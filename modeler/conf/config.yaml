# Here is all hyper parameters for transformer

# global settings
batch_size: 32
epochs: 1000

# lr schedule : cos annealing
lr_max: 6.0e-4
lr_min: 6.0e-6
Tw: 50   # Warmup steps
Tc: 900  # Cosine decay steps

# optimizer : AdamW
weight_decay: 0.01
beta1: 0.9
beta2: 0.999

# model settings
vocab_size: 10000
context_length: 256 # max sequence length
num_layers: 4
d_model: 512
num_heads: 16 # Make sure to double divide d_model
d_ff: 1344
rope_theta: 10000 # RoPE setting

# gradient clipping
grad_clip: 10


